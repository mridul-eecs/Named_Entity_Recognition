{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+2icNVLbs+io/NMaeyJFV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mridul-eecs/Named_Entity_Recognition/blob/main/transformer_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8CuiE-LVtP7"
      },
      "outputs": [],
      "source": [
        "! pip install spacy-transformers\n",
        "# annot: https://tecoholic.github.io/ner-annotator/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "nlp = spacy.blank(\"en\") # load a new spacy model\n",
        "db = DocBin() # create a DocBin object"
      ],
      "metadata": {
        "id": "Wb5c4flerxkn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spacy_docbin(file= 'train.json'):\n",
        "  f = open(file)\n",
        "  TRAIN_DATA = json.load(f)\n",
        "\n",
        "  for text, annot in tqdm(TRAIN_DATA['annotations']): \n",
        "      doc = nlp.make_doc(text) \n",
        "      ents = []\n",
        "      for start, end, label in annot[\"entities\"]:\n",
        "          span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "          if span is None:\n",
        "              print(\"Skipping entity\")\n",
        "          else:\n",
        "              ents.append(span)\n",
        "      doc.ents = ents \n",
        "      db.add(doc)\n",
        "  db.to_disk(\"./{}.spacy\".format(file.split('.')[0]))\n",
        "spacy_docbin('train.json')\n",
        "spacy_docbin('test.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXFMcyN9r-gT",
        "outputId": "ff7d8ee8-133c-4757-b952-ac4e109f2345"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 2200.22it/s]\n",
            "100%|██████████| 8/8 [00:00<00:00, 2269.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m spacy init config config.cfg --lang en --pipeline ner --optimize efficiency --gpu --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrAtQ08LsjGH",
        "outputId": "cde2bfe2-344f-4ecf-a0a4-24ed79e13ea5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
            "- Language: en\n",
            "- Pipeline: ner\n",
            "- Optimize for: efficiency\n",
            "- Hardware: GPU\n",
            "- Transformer: roberta-base\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m spacy train config.cfg --gpu-id 0 --output ./ --paths.train ./train.spacy --paths.dev ./test.spacy "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO5mu2BMs0eM",
        "outputId": "752dc89f-4f6e-4dc2-8325-b87a3f4b6001"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-08-02 14:34:39,990] [INFO] Set up nlp object from config\n",
            "[2022-08-02 14:34:40,001] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2022-08-02 14:34:40,005] [INFO] Created vocabulary\n",
            "[2022-08-02 14:34:40,006] [INFO] Finished initializing nlp object\n",
            "Downloading: 100% 481/481 [00:00<00:00, 483kB/s]\n",
            "Downloading: 100% 878k/878k [00:00<00:00, 5.04MB/s]\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 3.29MB/s]\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 7.60MB/s]\n",
            "Downloading: 100% 478M/478M [00:06<00:00, 74.1MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2022-08-02 14:35:00,721] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        3976.14    260.87    0.00    0.00    0.00    0.00\n",
            "200     200       43301.94  17008.04   95.33   95.33   95.33    0.95\n",
            "400     400           0.00   8816.85   95.33   95.33   95.33    0.95\n",
            "600     600           0.00   8416.29   95.33   95.33   95.33    0.95\n",
            "800     800          23.51   7938.57   95.33   95.33   95.33    0.95\n",
            "1000    1000         113.01   7400.91   95.33   95.33   95.33    0.95\n",
            "1200    1200           0.00   6620.55   95.33   95.33   95.33    0.95\n",
            "1400    1400           0.00   5817.93   95.33   95.33   95.33    0.95\n",
            "1600    1600           0.00   4941.21   95.33   95.33   95.33    0.95\n",
            "1800    1800           0.00   4017.17   95.33   95.33   95.33    0.95\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_ner = spacy.load(\"model-best\")\n",
        "doc = nlp_ner(\"users by division\")\n",
        "spacy.displacy.render(doc, style=\"ent\", jupyter=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "8ImFq5nxEnKD",
        "outputId": "7855b70f-d99b-499e-f12b-780896ac7b12"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    users\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CHARTNAME</span>\n",
              "</mark>\n",
              " by \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    division\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CHARTVIEW</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WixbshC3Euk6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}